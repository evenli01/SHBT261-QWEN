{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc3ENEvWY_Ru",
        "outputId": "c8cb4f14-46b5-4620-f893-64bfc57e97a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes datasets peft pillow tqdm \\\n",
        "              evaluate sentence-transformers rouge-score sacrebleu\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
        "RESULTS_DIR = \"results\"\n",
        "PLOTS_DIR = os.path.join(RESULTS_DIR, \"plots\")\n",
        "CKPT_DIR = \"checkpoints\"\n",
        "\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Global processor (reused)\n",
        "# -------------------------\n",
        "processor = Blip2Processor.from_pretrained(MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAqr5zDvZBmS",
        "outputId": "b11da809-b1bf-4ab0-b28d-b51da0a4cdee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# TextVQA Dataset\n",
        "# =========================\n",
        "class TextVQADataset(Dataset):\n",
        "    def __init__(self, split=\"train\", cache_dir=None, limit=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            split (str): \"train\", \"validation\", \"test\".\n",
        "            cache_dir (str, optional): cache dir.\n",
        "            limit (int, optional): subsample for quick runs.\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        print(f\"Loading TextVQA split: {split} ...\")\n",
        "        self.dataset = load_dataset(\"lmms-lab/textvqa\", split=split, cache_dir=cache_dir)\n",
        "        if limit is not None:\n",
        "            self.dataset = self.dataset.select(range(limit))\n",
        "        print(f\"Loaded {len(self.dataset)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item[\"image\"]                    # PIL Image\n",
        "        question = item[\"question\"]\n",
        "        image_id = item[\"image_id\"]\n",
        "        answers = item.get(\"answers\", [])\n",
        "        ocr_tokens = item.get(\"ocr_tokens\", [])\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"question\": question,\n",
        "            \"answers\": answers,\n",
        "            \"image_id\": image_id,\n",
        "            \"ocr_tokens\": ocr_tokens,\n",
        "        }\n",
        "\n",
        "\n",
        "def vqa_collate(batch):\n",
        "    return {\n",
        "        \"image\": [b[\"image\"] for b in batch],\n",
        "        \"question\": [b[\"question\"] for b in batch],\n",
        "        \"answers\": [b[\"answers\"] for b in batch],\n",
        "        \"image_id\": [b[\"image_id\"] for b in batch],\n",
        "        \"ocr_tokens\": [b[\"ocr_tokens\"] for b in batch],\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# Metrics\n",
        "# =========================\n",
        "try:\n",
        "    bleu_metric = evaluate.load(\"bleu\")\n",
        "    meteor_metric = evaluate.load(\"meteor\")\n",
        "    rouge_metric = evaluate.load(\"rouge\")\n",
        "except Exception as e:\n",
        "    print(\"Warning loading text metrics:\", e)\n",
        "    bleu_metric = meteor_metric = rouge_metric = None\n",
        "\n",
        "try:\n",
        "    semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
        "except Exception as e:\n",
        "    print(\"Warning loading semantic model:\", e)\n",
        "    semantic_model = None\n",
        "\n",
        "\n",
        "def preprocess_answer(ans):\n",
        "    ans = str(ans).lower()\n",
        "    ans = ans.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
        "    return ans\n",
        "\n",
        "\n",
        "def compute_vqa_accuracy(ground_truth_list, predicted_answer):\n",
        "    if not ground_truth_list:\n",
        "        return 0.0\n",
        "    predicted_answer = preprocess_answer(predicted_answer)\n",
        "    gts = [preprocess_answer(a) for a in ground_truth_list]\n",
        "    match_count = sum(1 for gt in gts if gt == predicted_answer)\n",
        "    return min(1.0, match_count / 3.0)  # 官方公式\n",
        "\n",
        "\n",
        "def compute_semantic_similarity(ground_truth_list, predicted_answer):\n",
        "    if semantic_model is None:\n",
        "        return 0.0\n",
        "    predicted_answer = str(predicted_answer)\n",
        "    gts = [str(gt) for gt in (ground_truth_list or [\"\"])]\n",
        "    pred_emb = semantic_model.encode(predicted_answer, convert_to_tensor=True)\n",
        "    gt_embs = semantic_model.encode(gts, convert_to_tensor=True)\n",
        "    cosine_scores = util.cos_sim(pred_emb, gt_embs)\n",
        "    if cosine_scores.numel() == 0:\n",
        "        return 0.0\n",
        "    return float(torch.max(cosine_scores).item())\n",
        "\n",
        "\n",
        "def calculate_metrics(results):\n",
        "    \"\"\"\n",
        "    results: list of {\n",
        "       \"predicted_answer\": str,\n",
        "       \"ground_truth_answers\": list[str]\n",
        "    }\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return {}\n",
        "\n",
        "    total_acc = 0.0\n",
        "    predictions, references = [], []\n",
        "    semantic_scores = []\n",
        "\n",
        "    for item in results:\n",
        "        pred = item[\"predicted_answer\"]\n",
        "        gts = item[\"ground_truth_answers\"]\n",
        "\n",
        "        acc = compute_vqa_accuracy(gts, pred)\n",
        "        total_acc += acc\n",
        "\n",
        "        predictions.append(str(pred))\n",
        "        references.append([str(gt) for gt in gts] if gts else [\"\"])\n",
        "\n",
        "        if semantic_model is not None:\n",
        "            semantic_scores.append(compute_semantic_similarity(gts, pred))\n",
        "\n",
        "    metrics = {\"accuracy\": total_acc / len(results)}\n",
        "\n",
        "    # BLEU\n",
        "    if bleu_metric is not None:\n",
        "        try:\n",
        "            bleu_score = bleu_metric.compute(\n",
        "                predictions=predictions,\n",
        "                references=references,\n",
        "                max_order=2,\n",
        "                smooth=True,\n",
        "            )\n",
        "            metrics[\"bleu\"] = float(bleu_score.get(\"bleu\", 0.0))\n",
        "        except Exception as e:\n",
        "            print(\"Error computing BLEU:\", e)\n",
        "            metrics[\"bleu\"] = 0.0\n",
        "\n",
        "    # METEOR\n",
        "    if meteor_metric is not None:\n",
        "        try:\n",
        "            ms = meteor_metric.compute(predictions=predictions, references=references)\n",
        "            metrics[\"meteor\"] = float(ms.get(\"meteor\", 0.0))\n",
        "        except Exception as e:\n",
        "            print(\"Error computing METEOR:\", e)\n",
        "            metrics[\"meteor\"] = 0.0\n",
        "\n",
        "    # ROUGE\n",
        "    if rouge_metric is not None:\n",
        "        try:\n",
        "            rs = rouge_metric.compute(predictions=predictions, references=references)\n",
        "            metrics[\"rouge1\"] = float(rs.get(\"rouge1\", 0.0))\n",
        "            metrics[\"rouge2\"] = float(rs.get(\"rouge2\", 0.0))\n",
        "            metrics[\"rougeL\"] = float(rs.get(\"rougeL\", 0.0))\n",
        "        except Exception as e:\n",
        "            print(\"Error computing ROUGE:\", e)\n",
        "            metrics[\"rouge1\"] = 0.0\n",
        "            metrics[\"rouge2\"] = 0.0\n",
        "            metrics[\"rougeL\"] = 0.0\n",
        "\n",
        "    if semantic_scores:\n",
        "        metrics[\"semantic_similarity\"] = float(sum(semantic_scores) / len(semantic_scores))\n",
        "    else:\n",
        "        metrics[\"semantic_similarity\"] = 0.0\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0rxoweuZDDT"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# OCR cleaning & question type\n",
        "# =========================\n",
        "\n",
        "def clean_ocr_text(ocr_tokens, max_tokens=15, min_token_length=2):\n",
        "    if not ocr_tokens:\n",
        "        return \"\"\n",
        "    cleaned = []\n",
        "    seen = set()\n",
        "    for token in ocr_tokens:\n",
        "        token = token.strip()\n",
        "        if not token or len(token) < min_token_length:\n",
        "            continue\n",
        "        alnum_ratio = sum(c.isalnum() for c in token) / len(token)\n",
        "        if alnum_ratio < 0.3:\n",
        "            continue\n",
        "        token_lower = token.lower()\n",
        "        if token_lower not in seen:\n",
        "            seen.add(token_lower)\n",
        "            cleaned.append(token)\n",
        "        if len(cleaned) >= max_tokens:\n",
        "            break\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "\n",
        "def classify_question(question):\n",
        "    q = question.lower()\n",
        "    brand_keywords = [\"brand\", \"logo\", \"label\", \"company\", \"manufacturer\", \"maker\"]\n",
        "    number_keywords = [\"number\", \"percent\", \"%\", \"price\", \"$\", \"cost\", \"amount\", \"how much\", \"how many\"]\n",
        "    date_keywords = [\"year\", \"date\", \"born\", \"since\", \"when\"]\n",
        "    time_keywords = [\"time\", \"clock\", \"what time\"]\n",
        "    text_keywords = [\"text\", \"say\", \"spell\", \"read\", \"what does\", \"what is written\"]\n",
        "\n",
        "    if any(k in q for k in brand_keywords):\n",
        "        return \"brand\"\n",
        "    if any(k in q for k in number_keywords):\n",
        "        return \"number\"\n",
        "    if any(k in q for k in date_keywords):\n",
        "        return \"date\"\n",
        "    if any(k in q for k in time_keywords):\n",
        "        return \"time\"\n",
        "    if any(k in q for k in text_keywords):\n",
        "        return \"text\"\n",
        "    return \"general\"\n",
        "\n",
        "\n",
        "def summarize_ocr_by_type(ocr_tokens, q_type, max_tokens=15, min_token_length=2):\n",
        "    if not ocr_tokens:\n",
        "        return \"\"\n",
        "    filtered = []\n",
        "    for token in ocr_tokens:\n",
        "        token = token.strip()\n",
        "        if not token or len(token) < min_token_length:\n",
        "            continue\n",
        "        alnum_ratio = sum(c.isalnum() for c in token) / len(token)\n",
        "        if alnum_ratio < 0.3:\n",
        "            continue\n",
        "\n",
        "        if q_type == \"brand\":\n",
        "            if token[0].isupper() and token.isalpha():\n",
        "                filtered.append(token)\n",
        "        elif q_type == \"number\":\n",
        "            if any(c.isdigit() for c in token) or \"$\" in token or \"%\" in token:\n",
        "                filtered.append(token)\n",
        "        elif q_type == \"date\":\n",
        "            if re.match(r\"^\\d{4}$\", token) or re.match(r\"^\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}$\", token):\n",
        "                filtered.append(token)\n",
        "            elif token.isdigit() and len(token) >= 4:\n",
        "                filtered.append(token)\n",
        "        elif q_type == \"time\":\n",
        "            if re.match(r\"^\\d{1,2}[:]\\d{2}\", token):\n",
        "                filtered.append(token)\n",
        "            elif any(c.isdigit() for c in token) and len(token) <= 5:\n",
        "                filtered.append(token)\n",
        "        elif q_type == \"text\":\n",
        "            filtered.append(token)\n",
        "        else:\n",
        "            filtered.append(token)\n",
        "\n",
        "        if len(filtered) >= max_tokens:\n",
        "            break\n",
        "\n",
        "    return \" \".join(filtered) if filtered else \"\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Prompt templates\n",
        "# =========================\n",
        "PROMPT_TEMPLATES = {\n",
        "    # Baselines (non-OCR)\n",
        "    \"default\": \"Question: {question} Answer:\",\n",
        "    \"descriptive\": \"Based on the image, answer the question briefly: {question}\",\n",
        "    \"instruction\": \"Look at the image and answer in a few words: {question}\",\n",
        "    \"direct\": \"{question}\",\n",
        "    \"text_focus\": \"Focus on any visible text in the image. Question: {question}\",\n",
        "    \"short_direct\": \"Answer in 1–3 words: {question}\",\n",
        "\n",
        "    # OCR-related\n",
        "    \"ocr_hint\": \"The image contains the following text: {ocr_text}. Question: {question} Answer:\",\n",
        "    \"ocr_hint_v3\": \"Answer this question about the image: {question}\\nVisible text in image: {ocr_text}\\nAnswer:\",\n",
        "    \"basic_ocr\": \"Detected text in the image: {ocr_text}\\nQuestion: {question}\\nAnswer in a short phrase:\",\n",
        "    \"ocr_category\": (\n",
        "        \"Relevant text in the image ({q_type}): {ocr_summary}\\n\"\n",
        "        \"Question: {question}\\n\"\n",
        "        \"Use ONLY that text when answering.\\n\"\n",
        "        \"Answer briefly:\"\n",
        "    ),\n",
        "    \"structured_ocr\": (\n",
        "        \"Relevant text in the image ({q_type}): {ocr_summary}\\n\"\n",
        "        \"Question: {question}\\n\"\n",
        "        \"Answer in 1–3 words using that text:\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "def build_formatted_question(question, ocr_tokens, template_name=None):\n",
        "    \"\"\"\n",
        "    prompt builder for BLIP\n",
        "    \"\"\"\n",
        "    if template_name is None:\n",
        "        return question\n",
        "\n",
        "    tpl = PROMPT_TEMPLATES.get(template_name, \"Question: {question} Answer:\")\n",
        "\n",
        "    ocr_text = \"\"\n",
        "    ocr_summary = \"\"\n",
        "    q_type = \"\"\n",
        "\n",
        "    if \"{ocr_summary}\" in tpl and \"{q_type}\" in tpl:\n",
        "        q_type = classify_question(question)\n",
        "        ocr_summary = summarize_ocr_by_type(ocr_tokens, q_type)\n",
        "        if not ocr_summary:\n",
        "\n",
        "            stripped = (\n",
        "                tpl.replace(\"Relevant text in the image ({q_type}): {ocr_summary}\\n\", \"\")\n",
        "                   .replace(\"Use ONLY that text when answering.\\n\", \"\")\n",
        "                   .replace(\"Answer in 1–3 words using that text:\", \"Answer briefly:\")\n",
        "            )\n",
        "            return stripped.format(question=question, q_type=q_type)\n",
        "        return tpl.format(question=question, ocr_summary=ocr_summary, q_type=q_type)\n",
        "\n",
        "    if \"{ocr_text}\" in tpl:\n",
        "        ocr_text = clean_ocr_text(ocr_tokens)\n",
        "        if not ocr_text:\n",
        "            stripped = (\n",
        "                tpl.replace(\"{ocr_text}\", \"\")\n",
        "                   .replace(\"Detected text in the image:\", \"\")\n",
        "                   .replace(\"Visible text in image:\", \"\")\n",
        "                   .replace(\"The image contains the following text:\", \"\")\n",
        "            )\n",
        "            return stripped.format(question=question)\n",
        "        return tpl.format(question=question, ocr_text=ocr_text)\n",
        "\n",
        "    # non OCR\n",
        "    return tpl.format(question=question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXASNeD8ZGbF"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# BLIP-2 OPT-2.7B Model Wrapper\n",
        "# =========================\n",
        "\n",
        "def load_blip2_opt(dtype=None):\n",
        "    print(\"Loading BLIP-2 OPT-2.7B ...\")\n",
        "    if dtype is None:\n",
        "        dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=dtype,\n",
        "    ).to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def blip_generate(model, image, prompt, max_new_tokens=20, num_beams=1):\n",
        "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    gen_kwargs = {\"max_new_tokens\": max_new_tokens}\n",
        "    if num_beams > 1:\n",
        "        gen_kwargs.update({\"num_beams\": num_beams})\n",
        "    output_ids = model.generate(**inputs, **gen_kwargs)\n",
        "    text = processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    if \"answer:\" in text.lower():\n",
        "        idx = text.lower().rfind(\"answer:\")\n",
        "        pred = text[idx + len(\"answer:\"):].strip()\n",
        "    else:\n",
        "        pred = text.strip()\n",
        "    return pred\n",
        "\n",
        "\n",
        "# =========================\n",
        "# LoRA\n",
        "# =========================\n",
        "\n",
        "def build_lora_model(base_model, r=16, alpha=16, target_modules=(\"q_proj\", \"v_proj\")):\n",
        "    cfg = LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        target_modules=list(target_modules),\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    lora_model = get_peft_model(base_model, cfg)\n",
        "    lora_model.print_trainable_parameters()\n",
        "    return lora_model\n",
        "\n",
        "\n",
        "def train_one_epoch_lora(model, train_loader, lr=1e-4, max_steps=300, prompt_template=\"default\"):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    step = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, total=max_steps):\n",
        "        images = batch[\"image\"]\n",
        "        questions = batch[\"question\"]\n",
        "        answers_list = batch[\"answers\"]\n",
        "\n",
        "\n",
        "        gt_answers = [a[0] if len(a) > 0 else \"\" for a in answers_list]\n",
        "\n",
        "\n",
        "        full_texts = [\n",
        "            f\"Question: {q} Answer: {ans}\"\n",
        "            for q, ans in zip(questions, gt_answers)\n",
        "        ]\n",
        "\n",
        "        enc = processor(\n",
        "            images=images,\n",
        "            text=full_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        ).to(device)\n",
        "\n",
        "        labels = enc[\"input_ids\"].clone()\n",
        "        enc[\"labels\"] = labels\n",
        "\n",
        "        out = model(**enc)\n",
        "        loss = out.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"step {step} | loss = {loss.item():.4f}\")\n",
        "\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "\n",
        "    avg_loss = running_loss / max_steps\n",
        "    print(f\"Average train loss: {avg_loss:.4f}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6JBRdSHZJ9o"
      },
      "outputs": [],
      "source": [
        "def save_results_json(model_name_prefix, zs_or_ft, prompt_name, split, metrics, results):\n",
        "\n",
        "    parts = [model_name_prefix]\n",
        "    if zs_or_ft == \"ft\":\n",
        "        parts.append(\"finetuned\")\n",
        "    if prompt_name is not None:\n",
        "        parts.append(f\"prompt_{prompt_name}\")\n",
        "    parts.append(split)\n",
        "    parts.append(\"results\")\n",
        "    fname = \"_\".join(parts) + \".json\"\n",
        "    out_path = os.path.join(RESULTS_DIR, fname)\n",
        "\n",
        "    payload = {\n",
        "        \"metrics\": metrics,\n",
        "        \"results\": results,\n",
        "        \"config\": {\n",
        "            \"model\": model_name_prefix,\n",
        "            \"zs_or_ft\": zs_or_ft,\n",
        "            \"prompt_name\": prompt_name,\n",
        "            \"split\": split,\n",
        "        },\n",
        "    }\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump(payload, f, indent=2)\n",
        "    print(f\"Saved: {out_path}\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_eval_blip(model, split=\"validation\", limit=200, prompt_template=None, zs_or_ft=\"zs\"):\n",
        "    ds = TextVQADataset(split=split)\n",
        "    loader = DataLoader(ds, batch_size=1, shuffle=False, collate_fn=vqa_collate)\n",
        "\n",
        "    results = []\n",
        "    count = 0\n",
        "\n",
        "    tag = f\"{zs_or_ft} {prompt_template or 'default'}\"\n",
        "    print(f\"Evaluating BLIP-2 OPT ({tag}) on {split}, limit={limit} ...\")\n",
        "\n",
        "    for batch in tqdm(loader, total=min(limit, len(ds))):\n",
        "        image = batch[\"image\"][0]\n",
        "        question = batch[\"question\"][0]\n",
        "        answers = batch[\"answers\"][0]\n",
        "        image_id = batch[\"image_id\"][0]\n",
        "        ocr_tokens = batch[\"ocr_tokens\"][0]\n",
        "\n",
        "        formatted_q = build_formatted_question(question, ocr_tokens, prompt_template)\n",
        "\n",
        "        try:\n",
        "            pred = blip_generate(model, image, formatted_q)\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {image_id}: {e}\")\n",
        "            pred = \"\"\n",
        "\n",
        "        item = {\n",
        "            \"image_id\": image_id,\n",
        "            \"question\": question,\n",
        "            \"formatted_question\": formatted_q,\n",
        "            \"predicted_answer\": pred,\n",
        "            \"ground_truth_answers\": answers,\n",
        "        }\n",
        "        results.append(item)\n",
        "\n",
        "        count += 1\n",
        "        if count >= limit:\n",
        "            break\n",
        "\n",
        "    metrics = calculate_metrics(results)\n",
        "    print(\"Metrics:\", metrics)\n",
        "\n",
        "    save_results_json(\n",
        "        model_name_prefix=\"blip_opt\",\n",
        "        zs_or_ft=zs_or_ft,\n",
        "        prompt_name=prompt_template if prompt_template else \"default\",\n",
        "        split=split,\n",
        "        metrics=metrics,\n",
        "        results=results,\n",
        "    )\n",
        "    return metrics, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NEjvwNOZMKi"
      },
      "outputs": [],
      "source": [
        "def load_blip_opt_records():\n",
        "    records = []\n",
        "    for fname in os.listdir(RESULTS_DIR):\n",
        "        if not fname.endswith(\".json\"):\n",
        "            continue\n",
        "        if not fname.startswith(\"blip_opt\"):\n",
        "            continue\n",
        "        path = os.path.join(RESULTS_DIR, fname)\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        metrics = data.get(\"metrics\", {})\n",
        "        base = os.path.splitext(fname)[0]\n",
        "        parts = base.split(\"_\")\n",
        "\n",
        "        model = parts[0]  # blip_opt\n",
        "        zs_or_ft = \"zs\"\n",
        "        prompt = \"none\"\n",
        "        split = \"validation\"\n",
        "\n",
        "        if \"finetuned\" in parts:\n",
        "            zs_or_ft = \"ft\"\n",
        "        if \"prompt\" in parts:\n",
        "            idx = parts.index(\"prompt\")\n",
        "            if idx + 1 < len(parts):\n",
        "                prompt = parts[idx + 1]\n",
        "        if parts[-1] == \"results\" and len(parts) >= 3:\n",
        "            split = parts[-2]\n",
        "\n",
        "        records.append({\n",
        "            \"file\": fname,\n",
        "            \"zs_or_ft\": zs_or_ft,\n",
        "            \"prompt\": prompt,\n",
        "            \"split\": split,\n",
        "            \"accuracy\": metrics.get(\"accuracy\", 0.0),\n",
        "            \"bleu\": metrics.get(\"bleu\", 0.0),\n",
        "            \"meteor\": metrics.get(\"meteor\", 0.0),\n",
        "            \"rouge1\": metrics.get(\"rouge1\", 0.0),\n",
        "            \"semantic_similarity\": metrics.get(\"semantic_similarity\", 0.0),\n",
        "        })\n",
        "    return records\n",
        "\n",
        "\n",
        "def plot_bar(records, metric, title, filename):\n",
        "    if not records:\n",
        "        print(f\"No records for metric {metric}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    records = sorted(records, key=lambda r: (r[\"zs_or_ft\"], r[\"prompt\"]))\n",
        "\n",
        "    labels, values = [], []\n",
        "    for r in records:\n",
        "        tag = \"ZS\" if r[\"zs_or_ft\"] == \"zs\" else \"FT\"\n",
        "        label = f\"{tag}-{r['prompt']}\"\n",
        "        labels.append(label)\n",
        "        values.append(r.get(metric, 0.0))\n",
        "\n",
        "    plt.figure(figsize=(max(8, len(labels) * 0.7), 5))\n",
        "    x = range(len(labels))\n",
        "    plt.bar(x, values)\n",
        "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = os.path.join(PLOTS_DIR, filename)\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"Saved plot: {out_path}\")\n",
        "\n",
        "\n",
        "def plot_all_blip_opt():\n",
        "    records = load_blip_opt_records()\n",
        "    if not records:\n",
        "        print(\"No blip_opt*.json results yet.\")\n",
        "        return\n",
        "\n",
        "    plot_bar(records, \"accuracy\", \"BLIP-2 OPT TextVQA Accuracy (ZS vs FT, prompts)\", \"blip_opt_accuracy.png\")\n",
        "    plot_bar(records, \"bleu\", \"BLIP-2 OPT BLEU (ZS vs FT, prompts)\", \"blip_opt_bleu.png\")\n",
        "    plot_bar(records, \"meteor\", \"BLIP-2 OPT METEOR (ZS vs FT, prompts)\", \"blip_opt_meteor.png\")\n",
        "    plot_bar(records, \"rouge1\", \"BLIP-2 OPT ROUGE-1 (ZS vs FT, prompts)\", \"blip_opt_rouge1.png\")\n",
        "    plot_bar(records, \"semantic_similarity\", \"BLIP-2 OPT Semantic Similarity (ZS vs FT, prompts)\", \"blip_opt_semantic.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aef82f7452994de5a740c52bf8114d79",
            "eff63d6083e244039cd53c2242576234",
            "f2dbc7658f3640ec9938fac14e31b43b",
            "69767c35d2734d0d9ed13614b0aad3ba",
            "01fdf1132cd346a2a5c0f6bc545a6759",
            "f2a9abfc6d014c44bf60125375290c6c",
            "7fd0c149004f45eba2ba4e4a844d3c6f",
            "aa6494e57fb645fd9d74fc4aaf13d6b2",
            "90ac3361877b444a96baff0b1fac49d3",
            "ecee5940d0f144eea99fd89bca156e44",
            "f3cf08ca3cac4f48a9edaaac3fce9059",
            "334dda6f4d8f40d89616c3b7074c8b01",
            "d8f162c169e448c086103c74da635443",
            "c293d8cfec514743906c68cfee44289b",
            "13611e6896ed4a2a8cd9248de821dbc4",
            "2660ba33f9e645a4baac93619b256ffe",
            "8adaf85c79024a768d345a48fb454ff9",
            "2391ff5297734d99b0d327aaf3b866ed",
            "67058f0ec0b94fcfb4b1f48ed0aefed5",
            "eeade51edba24688ac08ff6f61d62b3b",
            "44d20f37a1324988b6da777f33f6967d",
            "4164214675bf461e889c778ea3dd4d8f"
          ]
        },
        "id": "1pSa0CXCZN76",
        "outputId": "cb29fbda-aa90-4de1-ecde-480a7e413dd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BLIP-2 OPT-2.7B ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aef82f7452994de5a740c52bf8114d79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eff63d6083e244039cd53c2242576234",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2dbc7658f3640ec9938fac14e31b43b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69767c35d2734d0d9ed13614b0aad3ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01fdf1132cd346a2a5c0f6bc545a6759",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2a9abfc6d014c44bf60125375290c6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fd0c149004f45eba2ba4e4a844d3c6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TextVQA split: validation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa6494e57fb645fd9d74fc4aaf13d6b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90ac3361877b444a96baff0b1fac49d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecee5940d0f144eea99fd89bca156e44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3cf08ca3cac4f48a9edaaac3fce9059",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0/20 [00:00<?, ?files/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "334dda6f4d8f40d89616c3b7074c8b01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00020.parquet:   0%|          | 0.00/303M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8f162c169e448c086103c74da635443",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00020.parquet:   0%|          | 0.00/298M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c293d8cfec514743906c68cfee44289b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00002-of-00020.parquet:   0%|          | 0.00/290M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13611e6896ed4a2a8cd9248de821dbc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00003-of-00020.parquet:   0%|          | 0.00/304M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2660ba33f9e645a4baac93619b256ffe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00004-of-00020.parquet:   0%|          | 0.00/318M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8adaf85c79024a768d345a48fb454ff9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00005-of-00020.parquet:   0%|          | 0.00/262M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2391ff5297734d99b0d327aaf3b866ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00006-of-00020.parquet:   0%|          | 0.00/304M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67058f0ec0b94fcfb4b1f48ed0aefed5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00007-of-00020.parquet:   0%|          | 0.00/238M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eeade51edba24688ac08ff6f61d62b3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00008-of-00020.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44d20f37a1324988b6da777f33f6967d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00009-of-00020.parquet:   0%|          | 0.00/299M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4164214675bf461e889c778ea3dd4d8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00010-of-00020.parquet:   0%|          | 0.00/286M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "########################################\n",
        "# 1) Zero-shot baselines (BLIP-2 OPT)\n",
        "########################################\n",
        "blip_zs_model = load_blip2_opt()\n",
        "\n",
        "# ZS default\n",
        "zs_default_metrics, _ = run_eval_blip(\n",
        "    blip_zs_model,\n",
        "    split=\"validation\",\n",
        "    limit=200,\n",
        "    prompt_template=\"default\",\n",
        "    zs_or_ft=\"zs\",\n",
        ")\n",
        "\n",
        "# ZS basic_ocr\n",
        "zs_ocr_metrics, _ = run_eval_blip(\n",
        "    blip_zs_model,\n",
        "    split=\"validation\",\n",
        "    limit=200,\n",
        "    prompt_template=\"basic_ocr\",\n",
        "    zs_or_ft=\"zs\",\n",
        ")\n",
        "\n",
        "del blip_zs_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "########################################\n",
        "# 2) Fine-tune with LoRA (BLIP-2 OPT)\n",
        "########################################\n",
        "\n",
        "train_ds = TextVQADataset(split=\"train\", limit=5000)\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=vqa_collate)\n",
        "\n",
        "base_model = load_blip2_opt()\n",
        "lora_model = build_lora_model(base_model, r=16, alpha=16, target_modules=(\"q_proj\", \"v_proj\"))\n",
        "\n",
        "lora_model = train_one_epoch_lora(\n",
        "    lora_model,\n",
        "    train_loader,\n",
        "    lr=1e-4,\n",
        "    max_steps=300,\n",
        "    prompt_template=\"default\",\n",
        ")\n",
        "\n",
        "# save LoRA\n",
        "lora_save_path = os.path.join(CKPT_DIR, \"blip_opt_lora_r16_a16\")\n",
        "lora_model.save_pretrained(lora_save_path)\n",
        "print(\"LoRA saved to:\", lora_save_path)\n",
        "\n",
        "########################################\n",
        "# 3) Fine-tuned eval:\n",
        "########################################\n",
        "\n",
        "ft_default_metrics, _ = run_eval_blip(\n",
        "    lora_model,\n",
        "    split=\"validation\",\n",
        "    limit=200,\n",
        "    prompt_template=\"default\",\n",
        "    zs_or_ft=\"ft\",\n",
        ")\n",
        "\n",
        "ft_descr_metrics, _ = run_eval_blip(\n",
        "    lora_model,\n",
        "    split=\"validation\",\n",
        "    limit=200,\n",
        "    prompt_template=\"descriptive\",\n",
        "    zs_or_ft=\"ft\",\n",
        ")\n",
        "\n",
        "ft_textfocus_metrics, _ = run_eval_blip(\n",
        "    lora_model,\n",
        "    split=\"validation\",\n",
        "    limit=200,\n",
        "    prompt_template=\"text_focus\",\n",
        "    zs_or_ft=\"ft\",\n",
        ")\n",
        "\n",
        "ft_basicocr_metrics, _ = run_eval_blip(\n",
        "    lora_model,\n",
        "    split=\"validation\",\n",
        "    limit=200,\n",
        "    prompt_template=\"basic_ocr\",\n",
        "    zs_or_ft=\"ft\",\n",
        ")\n",
        "\n",
        "ft_structuredocr_metrics, _ = run_eval_blip(\n",
        "    lora_model,\n",
        "    split=\"validation\",\n",
        "    limit=200,\n",
        "    prompt_template=\"structured_ocr\",\n",
        "    zs_or_ft=\"ft\",\n",
        ")\n",
        "\n",
        "########################################\n",
        "# 4) plots\n",
        "########################################\n",
        "plot_all_blip_opt()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3fZeyYtZWOa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}