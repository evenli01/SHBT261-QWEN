# Training Configuration for Maximum Accuracy on TextVQA
# Target: 80-82% accuracy (realistic goal with Qwen2.5-VL-3B)
# Time: ~12-15 hours on A100 40GB

# Model configuration
model_name: "Qwen/Qwen2.5-VL-3B-Instruct"
model_cache_dir: "./models_cache"

# Dataset configuration
dataset_name: "lmms-lab/textvqa"
dataset_cache_dir: "./data_cache"
use_full_train_set: true
max_train_samples: null  # Use all 34.6k samples
max_eval_samples: 1000   # Limit eval for speed during training

# Training arguments
training_args:
  # Output
  output_dir: "./checkpoints/qwen_max_accuracy"
  overwrite_output_dir: false
  
  # Training epochs & batch size
  num_train_epochs: 10  # More epochs for better convergence
  per_device_train_batch_size: 2  # Smaller batch size = more updates
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8  # Effective batch size = 16
  
  # Learning rate & optimization
  learning_rate: 1.0e-5  # Conservative LR for stability
  lr_scheduler_type: "cosine"  # Cosine decay
  warmup_ratio: 0.1  # 10% warmup
  weight_decay: 0.01  # Regularization
  optim: "adamw_torch_fused"  # Fast fused optimizer
  
  # Precision & performance
  bf16: true  # Use bfloat16 for better precision
  fp16: false
  tf32: true  # Use TF32 on Ampere GPUs
  gradient_checkpointing: true  # Save memory
  max_grad_norm: 1.0  # Gradient clipping
  
  # Evaluation & checkpointing
  evaluation_strategy: "steps"
  eval_steps: 500  # Evaluate every 500 steps
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3  # Keep only best 3 checkpoints
  load_best_model_at_end: true
  metric_for_best_model: "accuracy"
  greater_is_better: true
  
  # Logging
  logging_dir: "./logs/qwen_max_accuracy"
  logging_strategy: "steps"
  logging_steps: 50
  report_to: ["tensorboard"]
  
  # Data loading
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  remove_unused_columns: false
  
  # Other
  seed: 42
  push_to_hub: false

# LoRA Configuration - Optimized for capacity
lora_config:
  r: 64  # Higher rank = more capacity
  lora_alpha: 128  # Alpha = 2*r recommended
  lora_dropout: 0.05  # Light dropout for regularization
  bias: "none"
  task_type: "CAUSAL_LM"
  
  # Target ALL attention and MLP layers for maximum adaptation
  target_modules:
    - "q_proj"
    - "k_proj" 
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  
  # Advanced LoRA options
  modules_to_save: null  # Don't train any full modules
  init_lora_weights: true
  use_rslora: false
  use_dora: false

# Data preprocessing
preprocessing:
  image_size: 448  # Qwen2.5-VL default
  max_length: 512  # Max text length
  padding: "max_length"
  truncation: true
  
# Generation config for evaluation
generation_config:
  max_new_tokens: 32  # TextVQA answers are short
  temperature: 0.1  # Low temp for deterministic answers
  top_p: 0.9
  do_sample: false  # Greedy decoding
  num_beams: 1  # No beam search during training eval (too slow)
  repetition_penalty: 1.0

# Early stopping (optional - enable if overfitting)
early_stopping:
  enabled: false
  patience: 3
  threshold: 0.001

# Advanced optimizations
optimizations:
  # Use Flash Attention 2 if available
  use_flash_attention_2: true
  
  # DeepSpeed ZeRO (optional - for multi-GPU)
  use_deepspeed: false
  deepspeed_config: null
  
  # Distributed training (if multi-GPU)
  ddp_find_unused_parameters: false
  
# Experiment tracking
experiment_name: "qwen2.5-vl-3b-textvqa-max-accuracy"
tags:
  - "textvqa"
  - "qwen2.5-vl"
  - "lora"
  - "max-accuracy"
notes: |
  Full training run optimized for maximum accuracy on TextVQA.
  - Using all 34.6k training samples
  - LoRA r=64 for high capacity
  - 10 epochs with cosine LR schedule
  - Conservative learning rate for stability
  - Expected accuracy: 78-82% on validation set
